<!DOCTYPE HTML>
<html>
  <head>
    <!-- Google analytics tag (gtag.js) -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-STGLQW4BJX"></script> -->
    <!-- <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-STGLQW4BJX');
    </script> -->

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0YKTTPW6Q2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-0YKTTPW6Q2');
    </script>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-PHCXQK3');</script>
    <!-- End Google Tag Manager -->
    <!-- Title -->

    <meta charset="utf-8" />
    <meta name="viewport" content="width=1000">

    <!-- Isotope JS -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.13.2/jquery-ui.min.js"></script>
    <script src="https://unpkg.com/isotope-layout@3/dist/isotope.pkgd.min.js"></script> -->



    <!-- Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
    <style>
      @import url('https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap');
    </style>
    <!-- Custom Style -->
    <link rel="stylesheet" href="style.css">
    <!-- <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"> -->
    <link rel="icon" href="./favicon_empty.ico" />
    <title>Lei Zhang</title>
    
  </head>

  <body id="body">
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PHCXQK3"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
    <div id="main">
      <div id="intro">
        <div id="intro-text">
          <h1 class="font-bold text-3xl" style="font-family: Georgia, serif;">Lei Zhang</h1>
          <p>
            <br>
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=I31H3UAAAAAJ">Google Scholar</a>
            &nbsp;&nbsp;&nbsp;
            <a href="https://github.com/leizhang-public">Github</a>
            &nbsp;&nbsp;&nbsp;
            <a href="https://www.linkedin.com/in/lei-zhang-cn-de">Linkedin</a>
            &nbsp;&nbsp;&nbsp;
            <a href="https://twitter.com/lzhang94">X/Twitter</a>
            &nbsp;&nbsp;&nbsp;
            <br>
            Email: zhanglei dot cn dot de at gmail dot com
            <br>

            Dexterous Robotic Manipulation; Computer Vision; Deep Learning; Reinforcement Learning
            <br><br>
            Education:<br>
            University of Hamburg (UHH, Germany) | Ph.D. Student<br>
            Leibniz Universit√§t Hannover (LUH, Germany) | M.Sc., Sep. 2017 - Mar. 2020<br>
            Harbin Institute of Technology (HIT, China) | B.Sc., Sep. 2012 - June. 2016
            <br>
            
          </p>
        </div>
        <div id="intro-image">
          <img src="images/lei-zhang-v1.jpeg">
        </div>
      </div>
      <!-- <div class="space-y-3 place-self-center sm:col-start-2 col-span-full"> -->

      <div id="filters" class="button-group">
        <button class="button is-checked" data-filter=".publication" style="font-family: Georgia, serif;">Selected Publications</button>
        <!-- <button class="button" data-filter=".misc">Misc</button> -->
      </div>

      <div class="grid">

        <!-- Publications -->
        
        <div class="list-item publication" data-category="publication">
          <a href="https://sites.google.com/view/cleardepth" class="thumbnail">
              <video playsinline="" muted="" autoplay="" loop="" width="180px">
                  <source src="images/2024-cleardepth-inference-results.mp4" type="video/mp4">
              </video>
          </a>
          <div class="project-description">
              <h3><a href="https://sites.google.com/view/cleardepth">
                ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation</a></h3>
              <p>
                <font color="808b96">Under Review, 2024</font><br>
                <!-- We developed a vision transformer-based algorithm for stereo depth recovery of transparent objects. This approach is complemented by an innovative feature post-fusion module, which enhances the accuracy of depth recovery by structural features in images. -->
                Kaixin Bai, Huajian Zeng, <b>Lei Zhang*</b>, Yiwen Liu, Hongli Xu, Zhaopeng Chen, Jianwei Zhang <br> 
                Keywords: Transparent Object; Stereo Perception.<br>
                  <a href="https://sites.google.com/view/cleardepth">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/abs/2409.08926">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/pdf/2409.08926">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://sites.google.com/view/cleardepth">Demo</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://sites.google.com/view/cleardepth">Dataset</a>
                  <br>
              </p>
              <p><br></p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://sites.google.com/view/ffh-cluttered-grasping" class="thumbnail">
              <video playsinline="" muted="" autoplay="" loop="" width="180px">
                  <source src="images/2024-FFHClutteredGrasping_scene7_combined_scene_grasp_quality_ten-crop.mp4" type="video/mp4">
              </video>
          </a>
          <div class="project-description">
              <h3><a href="https://sites.google.com/view/ffh-cluttered-grasping">
                FFHClutteredGrasping: Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping</a></h3>
              <p>
                <font color="808b96">Under Review, 2024</font><br>
                <!-- We introduce a novel contact semantic generation model CoSe-CVAE for estimating contact semantic map from object point cloud. After estimating the grasp candidates with grasp detection method, we propose an unified grasping evaluation model for estimate grasping quality for cluttered scenes based on the partial scene point cloud and sampled hand surface points. -->
                <b>Lei Zhang</b>, Kaixin Bai, Guowen Huang, Zhenshan Bing, Zhaopeng Chen, Alois Knoll, Jianwei Zhang <br>
                Keywords: Generation Model; Multi-fingered Robotic Hand; Grasping from Clutter Scenes.<br>
                  <a href="https://sites.google.com/view/ffh-cluttered-grasping">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/abs/2404.08844v2">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/abs/2404.08844v2">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://sites.google.com/view/ffh-cluttered-grasping">Demo</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://sites.google.com/view/ffh-cluttered-grasping">FFH Cluttered Grasping Dataset</a>
              </p>
              <p><br></p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://yuyangtu.github.io/projectToolEENet.html" class="thumbnail">
              <video playsinline="" muted="" autoplay="" loop="" width="180px">
                  <source src="images/2024-tooleenet_demo.mp4" type="video/mp4">
              </video>
          </a>
          <div class="project-description">
              <h3><a href="https://yuyangtu.github.io/projectToolEENet.html">
                ToolEENet: Tool Affordance 6D Pose Estimation</a></h3>
              <p>
                <font color="49bf9">2024 IEEE International Conference on Intelligent Robots and Systems (IROS2024)</font><br>
                <!-- We present the innovative TOOLEE dataset, which, to the best of our knowledge, is the first to feature affordance segmentation of a tool's end-effector (EE) along with its defined 6D pose based on its usage and we propose the ToolEENet framework for accurate 6D pose estimation of the tool's EE. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact-based manipulation scenarios. -->
                Yunlong Wang, <b>Lei Zhang</b>, Yuyang Tu, Hui Zhang, Kaixin Bai, Zhaopeng Chen, Jianwei Zhang <br>
                Keywords: 6D Pose Estimation, Multi-fingered Robotic Hand, Tool Use.<br>
                  <a href="https://yuyangtu.github.io/projectToolEENet.html">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/abs/2404.04193">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://github.com/yl-wang996/ToolEENet">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://drive.google.com/drive/u/0/folders/1GTUYQKX9m7vCOi_mMaqV0_3qmlNusxUT">Dataset</a>
                <br>
              </p>
              <p><br></p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
            <a href="https://leizhang-public.github.io/cg-cnn/" class="thumbnail">
                <video playsinline="" muted="" autoplay="" loop="" width="180px">
                    <source src="https://leizhang-public.github.io/cg-cnn/videos/success_example_02.mp4" type="video/mp4">
                </video>
            </a>
            <div class="project-description">
                <h3><a href="https://leizhang-public.github.io/cg-cnn/">
                  CG-CNN: A Collision-Aware Cable Grasping Method in Cluttered Environment</a></h3>
                <p>
                    <font color="49bf9">2024 IEEE International Conference on Robotics and Automation (ICRA2024)</font><br>
                    <b>Lei Zhang</b>, Kaixin Bai, Qiang Li, Zhaopeng Chen, Jianwei Zhang<br>
                    <!-- We introduce a Cable Grasping-Convolutional Neural Network (CG-CNN) designed to facilitate robust cable grasping in cluttered environments. Given our model‚Äôs implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. -->
                    Keywords: Collision Awareness; Grasping from Clutter Scenes with Complicated Objects; Sim-to-Real Tranfer.<br>
                    <a href="https://leizhang-public.github.io/cg-cnn/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://www.researchgate.net/publication/377772279_A_Collision-Aware_Cable_Grasping_Method_in_Cluttered_Environment">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://leizhang-public.github.io/cg-cnn/">Demo</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://leizhang-public.github.io/cg-cnn/">Cable Grasping Dataset</a>
                    <br>
                </p>
                <p><br></p>
            </div>
        </div>

        <div class="list-item publication" data-category="publication">
          <a href="https://baikaixin-public.github.io/structured_light_3D_synthesizer/" class="thumbnail">
              <video playsinline="" muted="" autoplay="" loop="" width="180px">
                  <source src="https://baikaixin-public.github.io/structured_light_3D_synthesizer/images/project_patterns.mp4" type="video/mp4">
              </video>
          </a>
          <div class="project-description">
              <h3><a href="https://baikaixin-public.github.io/structured_light_3D_synthesizer/">
                  Close the Sim2real Gap via Physically-based Structured Light Synthetic Data Simulation</a></h3>
              <p>
                <font color="49bf9">2024 IEEE International Conference on Robotics and Automation (ICRA2024)</font><br>
                Kaixin Bai, <b>Lei Zhang</b>, Zhaopeng Chen, Fang Wan, Jianwei Zhang <br>
                  <!-- We proposed a novel method to simulate structured light camera and generated realistic dataset for object detection, segmentation, robotic grasping tasks.<br> -->
                  Keywords: Structured-light Simulation for Generation of Realistic Data; Data Generation for Perception; Reduce Sim2Real Gap.<br>
                  <a href="https://baikaixin-public.github.io/structured_light_3D_synthesizer/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://baikaixin-public.github.io/structured_light_3D_synthesizer/">Demo</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="">Photorealistic Structured-light Dataset</a>
                  <br>
              </p>
              <p><br></p>
          </div>
      </div>

        <div class="list-item publication" data-category="publication">
            <a href="https://leizhang-public.github.io/towards_precise_model_free_robotic_grasping_with_sim_to_real_transfer_learning/" class="thumbnail">
                <video playsinline="" muted="" autoplay="" loop="" width="180px">
                    <source src="https://leizhang-public.github.io/towards_precise_model_free_robotic_grasping_with_sim_to_real_transfer_learning/images/02_multi_obj_grasp_youtube.mp4" type="video/mp4">
                </video>
            </a>
            <div class="project-description">
                <h3><a href="https://leizhang-public.github.io/towards_precise_model_free_robotic_grasping_with_sim_to_real_transfer_learning/">
                    Towards Precise Model-free Robotic Grasping with Sim-to-Real Transfer Learning</a></h3>
                <p>
                  <b>Lei Zhang</b>, Kaixin Bai, Zhaopeng Chen, Yunlei Shi, Jianwei Zhang<br>
                    2022 IEEE International Conference on Robotics and Biomimetics (IEEE ROBIO 2022).<br>
                    <font color="49bf9"><i>&#9733; Best Conference Paper Award Finalist, ROBIO &#9733;</i></font><br>
                    Keywords: Data augmentation for Generation of Dense Grasping Labels; Sim-to-Real Transfer; Model-free Grasping Dataset.<br>
                    <a href="https://leizhang-public.github.io/towards_precise_model_free_robotic_grasping_with_sim_to_real_transfer_learning/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/abs/2301.12249">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://leizhang-public.github.io/towards_precise_model_free_robotic_grasping_with_sim_to_real_transfer_learning/">Demo</a>
                </p>
                <p><br></p>
            </div>
        </div>
        
        <div class="list-item publication" data-category="publication">
            <a href="https://ieeexplore.ieee.org/abstract/document/10011669" class="thumbnail">
                <video playsinline="" muted="" autoplay="" loop="" width="180px">
                    <source src="images/2022-6d-pose-estimation.mp4" type="video/mp4">
                </video>
            </a>
            <div class="project-description">
                <h3><a href="https://ieeexplore.ieee.org/abstract/document/10011669">
                    Learning of 6D Object Poses with Multi-task Point-wise Regression Deep Networks</a></h3>
                <p>
                    Kaixin Bai, <b>Lei Zhang</b>, Zhaopeng Chen, Jianwei Zhang<br>
                    2022 IEEE International Conference on Robotics and Biomimetics (IEEE ROBIO 2022).<br>
                    Keywords: Deep Learning-based 6D Pose Estimation; 6D Rotation Representation; 6D Pose Estimation-based Robotic Grasping.<br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10011669">WebPage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                </p>
                <p><br></p>
            </div>
        </div>

        <div class="list-item publication" data-category="publication">
            <a href="https://ieeexplore.ieee.org/abstract/document/10011669" class="thumbnail">
                <video playsinline="" muted="" autoplay="" loop="" width="180px">
                    <source src="images/2021-hybrid-assembly-shi.mp4" type="video/mp4">
                </video>
            </a>
            <div class="project-description">
                <h3><a href="https://ieeexplore.ieee.org/abstract/document/9739349">
                    Maximizing the Use of Environmental Constraints: A Pushing-Based Hybrid Position/Force Assembly Skill for Contact-Rich Tasks</a></h3>
                <p>
                    Yunlei Shi, Zhaopeng Chen, Lin Cong, Yansong Wu, Martin Craiu-M√ºller, Chengjie Yuan, Chunyang Chang, <b>Lei Zhang</b>, Jianwei Zhang<br>
                    2022 IEEE International Conference on Robotics and Biomimetics (IEEE ROBIO 2021).<br>
                    Keywords: Contact-Rich Assembly; Hybrid Position/Force.<br>

                    <a href="https://arxiv.org/pdf/2208.06278.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                </p>
                <p><br></p>
            </div>
        </div>

        <!-- Services -->
        <!-- <div class="list-item misc" data-category="misc">
          <p class="date">Year</p>Part1, Part2 
        </div> -->
        <!-- <div class="list-item misc" data-category="misc">
          
        </div> -->

      </div>
      <div id="footer">Resume: <a href="files/Lei_Zhang_Resume.pdf" target="_blank">Download/View PDF</a>.</div>
      
      <div id="footer">Reference of this website: <a href="https://github.com/andyzeng/andyzeng.github.io">Andy Zeng's Website</a>.</div>

    </div>

    <script>

      // Isotope grid.
      var $grid = $('.grid').isotope({
        itemSelector: '.list-item',
        layoutMode: 'fitRows',
        transitionDuration: 0,
        stagger: 10,
        initLayout: false,
        getSortData: {
          name: '.name',
          symbol: '.symbol',
          number: '.number parseInt',
          category: '[data-category]',
          weight: function( itemElem ) {
            var weight = $( itemElem ).find('.weight').text();
            return parseFloat( weight.replace( /[\(\)]/g, '') );
          }
        }
      });

      // Bind filter button click.
      $('#filters').on( 'click', 'button', function() {
        var filterValue = $( this ).attr('data-filter');
        localStorage.setItem('filterValue', filterValue);
        $grid.isotope({ filter: filterValue });
      });

      // Change is-checked class on buttons.
      $('.button-group').each( function( i, buttonGroup ) {
        var $buttonGroup = $( buttonGroup );
        $buttonGroup.on( 'click', 'button', function() {
          $buttonGroup.find('.is-checked').removeClass('is-checked');
          $( this ).addClass('is-checked');
        });
      });

      function update_isotope() {
        // Retrieve cached button click.
        var defaultFilterValue = localStorage.getItem('filterValue');
        if (defaultFilterValue == null) {
          defaultFilterValue = ".highlight"
        }
        $grid.isotope({ filter: defaultFilterValue });
        var buttons = document.getElementsByClassName("button");
        for (var currButton of buttons) {
          if (currButton.getAttribute('data-filter') == defaultFilterValue) {
            currButton.classList.add('is-checked');
          } else {
            currButton.classList.remove('is-checked');
          }
        }
      }

      function toggle_bio() {
        var x = document.getElementById("more-bio");
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }

      function toggle_highlights() {
        var x = document.getElementById("main-highlights");
        var y = document.getElementById("more-highlights");
        var b = document.getElementById("toggle_highlights_button")
        if (y.style.display === "none") {
          x.style.display = "none";
          y.style.display = "block";
          b.innerHTML = "Show less"
          update_isotope();
        } else {
          x.style.display = "block";
          y.style.display = "none";
          b.innerHTML = "Show more"
          update_isotope();
        }
      }

      update_isotope();

    </script>
  </body>
</html>
